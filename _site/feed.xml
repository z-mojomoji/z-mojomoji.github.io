<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en" /><updated>2020-08-19T18:39:48+07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Moji Anusirikul</title><subtitle>Moji
</subtitle><author><name>Suphicha Anusirikul</name><email>suphicha.anu@gmail.com</email></author><entry><title type="html">Kaggle: TalkingData AdTracking Fraud Detection Challenge</title><link href="http://localhost:4000/project/2018-06-27-talkingdata/" rel="alternate" type="text/html" title="Kaggle: TalkingData AdTracking Fraud Detection Challenge " /><published>2018-06-27T00:00:00+07:00</published><updated>2018-06-27T00:00:00+07:00</updated><id>http://localhost:4000/project/talkingdata</id><content type="html" xml:base="http://localhost:4000/project/2018-06-27-talkingdata/">&lt;p&gt;&lt;a href=&quot;https://www.kaggle.com/&quot;&gt;Kaggle&lt;/a&gt; is a data science and machine learning competition platform, owned by Google, upon  which companies, including Facebook, Google, Quora, and others, publish their data and problems. &lt;!--more--&gt;Many data scientists, data-mining experts, and machine-learning engineers from all over the world, who call themselves “Kagglers” attempt to produce the most accurate predictions by analyzing the data and building predictive models.&lt;/p&gt;

&lt;p&gt;The contest that we took part in was the &lt;em&gt;&lt;a href=&quot;https://www.kaggle.com/c/talkingdata-adtracking-fraud-detection&quot;&gt;TalkingData AdTracking Fraud Detection Challenge&lt;/a&gt;&lt;/em&gt; held by Kaggle. &lt;a href=&quot;https://www.talkingdata.com/&quot;&gt;TalkingData&lt;/a&gt; is China’s largest independent big data service platform. Online advertisement companies, such as TalkingData, encounter click fraud often, resulting in inaccurate click data, and thus wasting both time and capital when it occurs in large volumes. TalkingData handles over 3 billion clicks per day, thus building an IP and device blacklist record of users who do not install apps after clicking on them by employing machine learning methods.&lt;/p&gt;

&lt;p&gt;The data presented by TalkingData contains information relating to users who click on advertisements, such as IP address’, apps, devices like iPhone, Xiaomi, and Huawei, operating systems like IOS or Android, channels, the time of the clicking, and so on. Our job was to employ the training data relating to circa 200 million clicks over a four day period, including the features mentioned above, in order to build a reasonable model with which to predict whether the click is fraudulent or not. The score will be determined by the precision of our prediction based on the testing data.&lt;/p&gt;

&lt;p class=&quot;message&quot;&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: You can see our &lt;strong&gt;full&lt;/strong&gt; experience and &lt;strong&gt;details&lt;/strong&gt; of our model including feature engineering, trainning algorithm and some blending skills in this &lt;a href=&quot;/assets/file/Kaggle_Report_updated_622.pdf&quot;&gt;report&lt;/a&gt;.&lt;/p&gt;

&lt;!-- 
&lt;style&gt;h4{ margin:0 auto}&lt;/style&gt; 

&lt;h2&gt;Table of Contents&lt;/h2&gt;
1. this unordered seed list will be replaced by toc as unordered list
{:toc}

## Introduction

The training dataset provided by Talkingdata was around 8G in size, covering approximately 200 million clicks occurring between Nov, 06, 2017 and Nov, 09 2017. We were tasked with using the dataset to predict malicious IP’s capable of producing high volumes of clicks but which never install said apps. 
The test dataset was over 700M, covering approximately 19million clicks taking place on 10/10/2017, from 4:00 to 15:00.

| Feature           | Description                                                |
|:------------------|:-----------------------------------------------------------|
| `IP`              | IP address                                                 |
| `App`             | app id provided by advertisers                             |
| `Device`          | user's mobile device id, such as iPhone 6, iPhone 7        |
| `OS`              | the operating system version id of the user mobile device  |
| `Channel`         | advertising channel id                                     |
| `Click_time`      | click time (UTC time), in the format yyyy-mm-dd hh:mm:ss   |
| `Attributed_time` | if the user downloads the app, this is the download time   |
| `Is_attributed`   | whether the app is downloaded, this is the target variable |
{:.stretch-table.dl-table}

Here, IP, app, device, OS, and channel are all classified variables and are processed by coding.

## Work Flow

## Pre-processing

### Exploratory Data Analysis

&lt;h4&gt;&lt;b&gt;Data Distribution Pattern&lt;/b&gt;&lt;/h4&gt;
By handling the whole dataset, we get the number of unique values of each feature (shown in Figure 3-1). We can then find that the uniqueness count of the IP is much more than that of the application, operating system, and channel. 

![3-1](/assets/attachment/posts/2018-6-27-talkingdata/3-1.png){:.lead data-width=&quot;800&quot; data-height=&quot;100&quot;}
Figure 3-1 The number of unique values of each feature
{:.figure}

The occurrences plots  IP, application, device, OS, and channel values (Figure 3-2), and thus we see that some operations are possibly “fraud” operations, as their occurrence time is greater than expected. 

Figure 3-2 Value counts for app, IP, device, OS, and channel
{:.figure}

In addition, Figure 3-3 shows us that the proportion of users who download the app versus those who do not is quite unbalanced. 

Figure 3-3 App Downloaded Compared with Not Downloaded
{:.figure}

&lt;h4&gt;&lt;b&gt;Data Time Series Pattern&lt;/b&gt;&lt;/h4&gt;
We employed click time features to build hourly patterns. Figure 3-4 displays the time patterns of clicks by the hour over the course of these days. The time pattern of hourly click frequency is definite, however the hourly time pattern in ratios is not as obvious as said click frequency. 

Figure 3-4 Hourly Click Frequency and Conversion Ratio
{:.figure}

Then we try to merge these four days together and extract the hour of day from each day as a separate feature (Figure 3-5). By checking the click count compared with the proportion converted each hour, we find that between 12:00 and 13:00 the number of clicks and the proportion converted are both high, and after that these values both drop. However, from 15:00 the proportion converted suddenly increases, but that click count continues going down, which is a strange phenomenon that may need further analysis. 

Figure 3-5 Click Count comparted with Proportion Converted
{:.figure}

### Feature Engineering

&lt;h4&gt;&lt;b&gt;Click Time&lt;/b&gt;&lt;/h4&gt;
We first transpose ’Click_time’ into day and hour categorical features. We do not believe that the details, click minute and second, are helpful within the classification task, and therefor  don’t include them.

&lt;h4&gt;&lt;b&gt;Groupby Related&lt;/b&gt;&lt;/h4&gt;
We groupby some of the original categorical data (E.g., group samples by the ‘IP’, ‘app’, ‘channel’), and then get unique values, variances,  counts, and the cumulative counts of that group. 

| No.  | Method                                                                           |
|:-----|:---------------------------------------------------------------------------------|
| `X0` | Group by ['IP', 'channel'], calculate the number of unique items                 |
| `X1` | Group by ['IP', 'device', 'OS', 'app'],  and calculate the cumulative count      |
| `X2` | Group by ['IP', 'day', 'hour'], and calculate the number of unique items         |
| `X3` | Group by ['IP', 'app'], and calculate the number of unique items.                |
| `X4` | Group by ['IP', 'app', 'OS'], and calculate number of unique items.              |
| `X5` | Group by ['IP', 'device'], and calculate the number of unique items              |
| `X6` | Group by ['app', 'channel'], and calculate the number of unique items            |
| `X7` | Group by ['IP', 'OS'], and calculate the cumulative count                        |
| `X8` | Group by ['IP', 'device', 'OS', 'app'], and calculate the number of unique items |
| `F0` | Group by ['IP', 'day', 'hour'], and calculate the count                          |
| `F1` | Group by ['IP', 'app'], and calculate the count                                  |
| `F2` | Group by ['IP', 'app', 'OS'], and calculate the count                            |
| `F3` | Group by ['IP',' day', 'channel'], and calculate the variance                    |
| `F4` | Group by ['IP', 'app', 'OS'], and calculate the variance                         |
| `F5` | Group by ['IP', 'app', 'channel'], and calculate the variance                    |
| `F6` | Group by ['IP', 'app', 'channel'], and calculate the mean                        |
{:.stretch-table.dl-table}

&lt;h4&gt;&lt;b&gt;Next Click&lt;/b&gt;&lt;/h4&gt;
Since repeated clicks from the same IP address is highly likely a fraud click, we calculate the nextClick attribute within the first group by employing some categorical features and then calculating the differences between ‘clicktimes’ for the next sample within the group. 

### Correlation
The correlation matrix plot of our added numerical data is shown below. From the figure we can deduce that many of them are highly correlated (i.e., correlation coefficients larger than 0.5). The main reasons are that those numerical features are all generated by the original 5 categorical features, so they are internally correlated. Meanwhile, the high correlation amongst features is one reason for us to apply highly non-linear models such as neural networks and CART based on the high correlation among features. 


## Learning Algorithm Training

### Overview of Machine Learning Methods

We have tried a number of different machine learning methods. Below is a quick review of the methods with a general description, their pros and cons.

### Light GBM

Conventional implementations of GBDT need to, for every feature, scan all the data instances in order to estimate the information gain of all the possible split points, which makes these implementations very time consuming when handling big data. Light GBM proposes two novel techniques: Gradient-based One-Side Sampling (GOSS) and Exclusive Feature Bundling (EFB) with which to tackle the challenge in the tradeoff between accuracy and efficiency by improving both of them. 

With GOSS, the model can exclude a significant proportion of data instances with small gradients, and only uses the rest to estimate the information gain. Since the data instances with larger gradients play a more important role in the computation of information gain, GOSS can obtain comparatively accurate estimations of the information gain with a much smaller data size. With EFB, it bundles mutually exclusive features in order to reduce the number of features. Using a greedy algorithm can achieve superior  approximation ratios when finding the optimal bundling of exclusive features.

So Light GBM is faster and more accurate than traditional tree models. It reduces the calculation cost of split gain and uses histogram subtraction in order to optimize both speed and memory usage. Additionally,  Light GBM grows tree based on Leaf-wise (Best-first) Tree Growth algorithms and supports categorical features because of the optimal split for categorical Features.
Since four of five features within the data set are categorical features, Light GBM is really an excellent machine learning algorithm.

Here’s several parameters with which to tune the Light GBM. 

| PARAMETER         | NOTE                                                   |  RANGE　   | CHOSEN VALUE |
|:------------------|:-------------------------------------------------------|:-----------|:-------------|
| learning_rate     | shrinkage rate                                         | [0, 1]     | 0.05, 0.1    |
| num_leaves*       | number of leaves in one tree                           | [4, 31]    | 7, 15, 31    |
| max_depth*        | limit the max depth for tree model                     | [3, 5]     | 4, 5         |
| min_child_samples | minimum number of data need in a child                 | 100        | 100          |
| max_bin           | number of bucketed bin for feature values              | 100        | 100          |
| subsample*        | subsample ratio of the training instance               | [0.7, 0.9] | 0.7, 0.9     |
| subsample_freq    | frequency of subsample                                 | 1          | 1            |
| colsample_bytree* | subsample ratio of columns when constructing each tree | [0.7, 0.9] | 0.7, 0.9     |
| min_child_weight  | minimum sum of instance weight needed in a child       | 0          | 0            |
| scale_pos_weight* | weight of positive class in binary classification task | [99, 400]  | 200          |
{:.stretch-table.dl-table}
 --&gt;</content><author><name>Suphicha Anusirikul</name><email>suphicha.anu@gmail.com</email></author><category term="kaggle" /><summary type="html">Kaggle is a data science and machine learning competition platform, owned by Google, upon which companies, including Facebook, Google, Quora, and others, publish their data and problems.</summary></entry></feed>